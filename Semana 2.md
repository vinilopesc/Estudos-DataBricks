# Relatório de Estudos – Semana 2

## Dia 6
No primeiro dia desta semana, dediquei a manhã para compreender a fundo o que é o Spark SQL e por que ele se destaca na análise de dados distribuída. Eu entendi que o Spark SQL funciona como um módulo capaz de processar consultas em dados enormes, convertendo essas consultas em planos de execução altamente otimizados. Isso traz vantagens como velocidade, escalabilidade e facilidade de escrever código em diferentes linguagens (Python, Scala, SQL). Nesse processo, percebi que DataFrames e Datasets são estruturas fundamentais: os DataFrames oferecem uma visão tabular dos dados, facilitando consultas e transformações com uma sintaxe parecida com SQL, enquanto os Datasets (mais comuns em Scala) adicionam um forte sistema de tipos, ajudando a identificar problemas antes mesmo de executar o job.

Durante a tarde, foquei nas consultas básicas, testando o Spark SQL em cenários reais. Pude perceber que, mesmo com instruções simples como projeções (selecionar colunas) e filtros (`WHERE` e `HAVING`), o Spark otimiza tudo “por baixo dos panos”. Essa forma de trabalho me mostrou que, para grande parte das necessidades de análise, o Spark SQL reduz bastante o tempo de desenvolvimento, pois não preciso controlar manualmente como os dados vão se distribuir pelo cluster. Consegui enxergar, na prática, o quanto essa camada de abstração é vantajosa para agilizar o trabalho de engenharia de dados.

## Dia 7
No dia seguinte, aprofundei meus conhecimentos explorando funções avançadas do Spark SQL, como agregações mais complexas e funções de janela. Percebi que, utilizando comandos do tipo `GROUP BY`, consigo resumir os dados em métricas como soma, média ou contagem de forma fácil e extremamente rápida, mesmo com grandes volumes. Também entendi a importância de joins e como escolher o tipo certo (por exemplo, broadcast join versus sort merge join) faz diferença no desempenho, já que o Spark pode distribuir o processamento de maneira inteligente entre os nós do cluster.

À tarde, trabalhei com dados em formatos mais complexos, como JSON e estruturas aninhadas. Foi desafiador no começo, pois precisei manipular campos dentro de objetos e arrays, mas também foi recompensador perceber que o Spark oferece funções específicas para tratar esses formatos. Assim, pude extrair as informações necessárias sem precisar de um processo manual demorado. Reconheci que, em cenários reais, os dados muitas vezes não estão limpos ou em tabelas convencionais, e esse conhecimento é essencial para lidar com a variedade de formatos que encontramos no dia a dia.

## Dia 8
No terceiro dia da semana 2, iniciei meus estudos em PySpark, que é a interface Python para o Spark. Passei a manhã configurando o ambiente, instalando as dependências e entendendo a função do `SparkSession`, que é a entrada principal para trabalhar tanto com RDDs quanto com DataFrames. Esse passo foi fundamental para que eu pudesse começar a desenvolver aplicações que rodam localmente ou em clusters sem precisar alterar muito o código.

Na parte da tarde, comparei RDDs e DataFrames. Descobri que os RDDs dão mais liberdade de controle sobre a distribuição dos dados, mas exigem mais detalhes de implementação e são menos otimizados. Já os DataFrames permitem escrever consultas em alto nível, além de contar com otimizações automáticas feitas pelo Catalyst Optimizer. Entendi que a escolha entre um e outro depende do tipo de tarefa: se for mais analítica, DataFrames costumam ser a melhor opção; se for algo muito especializado e de baixo nível, os RDDs podem ter espaço.

## Dia 9
No quarto dia, apliquei diretamente o que aprendi sobre PySpark para criar fluxos de ETL. Pela manhã, concentrei esforços na limpeza e transformação de dados. Esse processo incluiu remover valores nulos, padronizar tipos e criar colunas derivadas que representam novas informações que antes não estavam explícitas. Enquanto fazia isso, percebi que a facilidade de lidar com grandes volumes em paralelo faz com que essas transformações sejam mais rápidas em comparação a ferramentas tradicionais.

À tarde, estudei técnicas de otimização dentro do Spark. Aprendi a usar `cache` e `persist` em DataFrames para evitar recalcular etapas que se repetem ao longo do pipeline, o que impacta diretamente o tempo de execução das tarefas. Também entendi como o particionamento de dados (partitioning) pode melhorar a performance, pois permite que o Spark leia somente os dados realmente necessários para a consulta. Nesse ponto, ficou claro que o sucesso de um projeto Spark não está apenas em saber escrever código, mas também em saber como otimizar e planejar as etapas do pipeline de dados.

## Dia 10
No último dia desta semana, explorei formas de integrar o Spark SQL dentro das aplicações PySpark. Criei tabelas temporárias, rodei consultas SQL diretamente em DataFrames e notei como é prático alternar entre a linguagem SQL e a API de DataFrames. Isso é útil quando algumas transformações são mais intuitivas em SQL, enquanto outras se beneficiam da flexibilidade do Python.

À tarde, consolidei tudo por meio de exercícios que simulavam situações reais. Os desafios envolviam juntar várias fontes de dados diferentes, fazer agregações complexas, manipular dados sujos e otimizar a execução para grandes volumes. Senti que essa prática me ajudou a fixar boa parte do aprendizado, principalmente porque me vi diante de problemas muito parecidos com os que ocorrem no mundo real. Concluir a semana dessa forma me deixou confiante para aprofundar o tema de ETL nas semanas seguintes.
